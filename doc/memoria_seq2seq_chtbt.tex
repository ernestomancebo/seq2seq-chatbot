\documentclass[12pt, letterpaper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[sorting=none]{biblatex}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{array}

% Bibliography
\addbibresource{./bib/Ma_2020.bib}
\addbibresource{./bib/rnn-effectiveness.bib}
\addbibresource{./bib/Understanding_LSTM.bib}
\addbibresource{./bib/Ramamoorthy.bib}
\addbibresource{./bib/Ramamoorthy2.bib}
\addbibresource{./bib/bahdanau2014neural.bib}
\addbibresource{./bib/a_tool_of_conversation.bib}
\addbibresource{./bib/racketracer.bib}
\addbibresource{./bib/Techlabs_2018.bib}
\addbibresource{./bib/tensorlayer2017.bib}
\addbibresource{./bib/nltk.bib}
\addbibresource{./bib/Kostadinov_2019.bib}
\addbibresource{./bib/Vinyals_Kaiser_Koo_Petrov_Sutskever_Hinton_2015.bib}
\addbibresource{./bib/google_s2s.bib}
\addbibresource{./bib/tf_softmax.bib}
\addbibresource{./bib/Khandelwal_2020.bib}
\addbibresource{./bib/tf_wb.bib}
\addbibresource{./bib/gfg_seq2seq.bib}
\addbibresource{./bib/Britz_2015.bib}
\addbibresource{./bib/tf_masking_padding.bib}
\addbibresource{./bib/greedy.bib}
% Parragraph formatting
\setlength{\parskip}{1em}

\title{Implementación de un Chatbot utilizando una Red Neuronal Seq2Seq}

\author{Ernesto Mancebo Tavárez \and Nathaniel Calderón González}
\affil{Procesamiento del Lenguaje Natural, MULCIA \\ Universidad de Sevilla, Sevilla, España}

\date{Junio 2020}

\begin{document}
    \begin{titlepage}
        \maketitle
        \begin{abstract}
            A continuación se presenta la memoria de un estudio sobre la implementación de un chatbot mediante el uso de una red neuronal bajo la arquitectura Seq2Seq, la cual es entrenada utilizando un corpus de mensajes extraídos de la red social Twitter.
        \end{abstract}
    \end{titlepage}

    \tableofcontents
    \pagebreak

    \section{Objeto de Estudio}
    \textbf{Seq2Seq Chatbot}, es un proyecto de aprendizaje profundo cuyo objetivo es crear un modelo de ML basado en la arquitectura de red Seq2Seq, donde tal modelo espera recibir una secuencia de entrada, o de forma práctica el inicio de una interlocución, y a partir de ella la red acuñará un contexto para posteriormente emitir una secuencia de salida o respuesta. Cabe destacar que el modelo es entrenado con un corpus de pregutnas y respuestas en el idioma inglés obtenido de la red social Twitter, es por ello que el domino del modelo se inclina hacia un diálogo conversacional.
    
    \section{Marco Teórico}
    Para apreciar íntegramente cada detalle del objeto de estudio, es preciso conocer conceptos que a continuación se puntualizarán.

    \subsection{Redes Recurrentes (RNN)}
        Las Redes Recurrentes son un tipo de red neuronal diseñadas especialmente para trabajar con data secuencial, tal data emite valores a lo largo del tiempo y estos gana significado a partir de sus valores previos. A modo de ejemplificar podemos incluir dentro de este tipo de data textos y videos. Estas redes se caracterizan por:
        
        \begin{itemize}
            \item Compartir parámetros.
            \item Capacidad de procesar secuencias de distintas longitudes.
            \item Detectar información relevante que pueden aparecer en distintas posiciones de la secuencia.
        \end{itemize}
    
        Las RNN a pesar de que no conocen el significado de los símbolos que procesan, éstas son capaces de inferir su significado a partir de la extructura de la secuencia y la posición de los mismos. Esta capacidad de poder contextualizar y recordar eventos previos de una secuencia, es gracia a que tales redes poseen una celda estado lo que en ocasiones se le puede escuchar referir tambien como \emph{memoria}.
    
        El estado o memoria de tal red se logra a partir del hecho de que una RNN son redes con bucles dentro de ella misma, lo cual hace que en cada llamada a sí misma pues proporciene la información procesada en la computación previa.
    
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.2\linewidth]{img/RNN-rolled.png}
            \caption{Ilustración de una RNN.}
        \end{figure}
    
        En la representación se tiene un trozo de red $A$, la cual toma como entrada un $x_t$ y emite un valor $h_t$, de este modo, utilizando el bucle mencionado la información pasa al siguiente paso dentro de la red. Dicho ya esto, podemos ilustrar una red recurrente como una red con múltiples copias de sí misma\cite{Understanding_LSTM}.
    
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{img/RNN-unrolled.png}
            \caption{Ilustración de la evolución de una RNN a través del tiempo $t$.}
        \end{figure}
        Esta manera de procesar la información es lo que hace que una RNN pueda procesar data secuencial y sea idónea para problemas tales como reconocimiento de voz, modelado del lenguaje, traducción, contextualización de imágenes y otras aplicaciones relacionadas\cite{rnn-effectiveness}.

    \subsection{Gated Recurrent Unit (GRU)}
    Las redes \textit{Gated Recurrent Unit}, en lo adelante GRU, son un tipo de red recurrente (RNN) más sofisticadas que las RNN tradicionales y no sufren los problemas de estas, tales como desvanecimiento del gradiente, un problema típico en las RNN. Para mitigar estos problemas las redes GRU utilizan dos compuertas que caracterizan estas redes:
    \begin{itemize}
        \item Compuerta de actualización \emph{(Update Gate)}.
        \item Compuerta de reinicio \emph{(Reset Gate)}.
    \end{itemize}
    
    Dichas compuertas representan dos vectores en la red que deciden qué información retener y qué información descartar respecto al vector de información de salida de la red.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{img/gru_net.png}
            \caption{Ilustración de una red GRU.}
        \end{figure}

    La notación utilizada en una red GRU corresponde a la terminología común manejada en el área de aprendizaje automático, las cuales se definen como:
    
    \begin{itemize}
        \item $\sigma$: Función Sigmoide.
        \item $\text{tanh}$: Función Tangente Hiperbólica.
        \item $\odot$: Operación Producto Hadamard.
        \item $+$: Operación Adición. 
    \end{itemize}

    Conocida ya la notación particular para la red, enlistamos los componentes internos de una celda GRU:  

    \begin{itemize}
        \item $z_t$ representa la compuerta de actualización, la cual se calcula a partir de multiplicar el peso de la compuerta por $x_t$ y $h_{t-1}$, luego sumar los resultados y aplicar la función sigmoide.
        \begin{equation}
            z_t=\sigma(W^zx_t+U^zh_{t-q})
        \end{equation}
        \item $r_t$ representa la compuerta de reinicio, básicamente se calcula igual que la compuerta de actualización, solo que con sus pesos.
        \begin{equation}
            r_t=\sigma(W^rx_t+U^rh_{t-1})
        \end{equation}
        \item Entonces, la forma en que interviene la compuerta de reinicio para determinar qué información descartar de la celda estado o $h_t'$ es calculada de la siguiente forma:
        \begin{equation}
            h_t'=\text{tanh}(Wx_t + r_t\odot{Uh_{t-1}})
        \end{equation}
        Se multiplica $x_t$ por el peso $W$, y lo mismo para $h_{t-1}$ por el peso $U$ donde luego este resultado se usa para calcular el producto \emph{Hadamard} con $r_t$. Por último se suman ambos resultados y se aplica la tangente hiperbólica. Básicamente cuando $r_t$ tiende a $0$ pues significa que se debe descartar tal información.

        \item Finalmente, para calcular la celda estado $h_t$ se utiliza la fórmula:
        \begin{equation}
            h_t = z_t\odot{h_{t-1} + (1-z_t)\odot{h_t}}
        \end{equation}
        
        Donde se calcula el producto de Hadamard para la compuerta de actualización y la celda estado previa, y el producto de \emph{Hadamard} para la misma compuerta de actualización y la celda estado $h_t'$. En esencia si el vector $z_t$ tiende a uno pues retendrá la mayoría de la información previa y por consiguiente ignorando gran parte de la información actual\cite{Kostadinov_2019}.
    \end{itemize}

    \subsection{Redes Seq2Seq}
    Una implementación \textit{\textbf{Seq2Seq}} consiste en dos redes recurrentes (RNN) enlazadas, actuando una como codificador y otra como decodificador. La tarea del codificador consiste en leer una secuencia de entrada y emitir un contexto a partir del último estado oculto $h_t$, utilizando sólo aquello que considere importante en la secuencia de entrada. Posteriormente, el decodificador extrae las caracteristicas aprendidas del codificador, al recibir como entrada los estados del mismo, generando asi palabra por palabra una distribución de probabilidad del vocabulario hasta formar la secuencia de salida\cite{Ramamoorthy}\cite{Ramamoorthy2}.

    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{img/Seq2Seq.png}
        \caption{Estructura de una red Seq2Seq.}
    \end{figure}

    Se aprecia en la parte central de un modelo Seq2Seq lo que se conoce como \emph{Though Vector} o vector de pensamiento, el cuál representa la intención o contexto de la secuencia de entrada y a partir de aquí la sección decodificadora del modelo genera una secuencia a emitir, símbolo tras símbolo, palabra en este caso, donde cada símbolo es influenciado por el anterior.
        
    \subsection{Greedy Search}
    Es un algoritmo de búsqueda que permite maximizar la probabilidad de predición, el cual consiste en basicamente seleccionar el mejor candidato de una distribución de probabilidad\cite{BibEntry2020Jun}.
    
    \subsection{Beam Search}
    Consiste en un algoritmo de búsqueda similar al Greedy Search, donde mientras Greedy Search siempre considera una única alternativa como la mejor, este algoritmo organiza las predicciones en forma de árbol y expande los nodos con mayor probabilidad.  Es necesario definir el Beam With o Top N lo cual limita a un número $n$ el número de alternativas que serán evaluadas en paralelo\cite{Khandelwal_2020}.
        
    \subsection{Contextualización de Palabra (Word Embedding)}
    La contextualización de palabras o word embedding, es una estrategia que permite vectorizar las palabras de un vocabulario de forma eficiente y en la que la codificación preserva la relación entre palabras. La contextualización de palabras es realizada usando una matriz de $n$ dimensiones de números reales, los cuales representan pesos entrenables que son aprendidos durante el entrenamiento, de la misma forma que son aprendidos los pesos de una red. 
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/4dim_embedding2.png}
        \caption{Ilustración de una matríz de Word Embedding.}
    \end{figure}
    A cada palabra del vocabulario del dataset se le asigna un índice en la matriz el cual es utilizado para referenciar la contextualización o \emph{“embedding"} a dicha palabra. Por esta razón se suele interpretar esta capa como una tabla de búsqueda o \emph{“lookup table"}\cite{tf_wb}.
    
Se puede valorizar cada palabra como un punto en un espacio y tal espacio es representado por el vector ya referido. Relaciones semanticas entre palabras son obtenidas mediante ésta técnica pues tienen propiedades aproximadas.
    
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{img/we1.png}
            \caption{Word Embedding.}
        \end{figure}
    
        Ésta tarea es típicamente aplicada en la primera fase de la red y esto se conoce como \emph{Embeding Layer}, donde se asocia una palabra (index to word in vocabulary) del vocabulario para un vector del tamaño dado. Para la arquitectura Seq2Seq los pesos de la capa \emph{Embeding Layer} son entrenados en conjunto con otros parámetros del modelo\cite{Ramamoorthy}.

    \subsection{Tecnologías Utilizadas}
        \subsubsection{TensorLayer}
        Es una librería consistente en una extensión o una capa de alto nivel para la librería TensorFlow de Google. Esta librería al igual que TensorFlow permite realizar aprendizaje profundo \emph{(Deep Learning)} y aprendizaje por refuerzo \emph{(Reinforcement Learning)} pero a un nivel de abstracción superior, simplificando muchas de las tareas que comúnmente realizamos con TensorFlow.
        
        TensorLayer proporciona módulos de Aprendizaje Profundo y Aprendizaje por Refuerzo populares listos para ser usados, los cuales pueden ser modificados y adaptados a necesidades específicas de forma simple. Tal es el caso de la arquitectura de red Seq2Seq, la cual viene ya implementada y disponible en la libreria, y es la utilizada en este proyecto con ligeras modificaciones.
        
        También es importante destacar que a pesar de TensorLayer ser una librería de alto nivel, la misma no tiene efectos sobre el rendimiento de TensorFlow, por lo que además de ser una libreria  simple y flexible, esto la hace aún más atractiva para la comunidad de científicos de datos\cite{tensorlayer2017}.

        \subsubsection{NLTK}
        NLTK o Natural Language Toolkit, es una plataforma para trabajar con procesamiento del lenguaje natural en Python. NLTK cuenta con una coleccion de librerias para las típicas tareas de procesamiento de texto tales como, clasificación de textos, tokenización, análisis gramatical (parsing), razonamiento semántico, entre otros. NLTK también ofrece incorporado soporte a diferentes corpus y modelos de Aprendizaje Automático ya entrenados\cite{nltk}.

    \section{Estado del Arte}
    Se presenta a continuación una tabla comparativa del estado del arte chatbots, tals chatbos son contemporáneos con el estudiado en el presente proyecto estudiado, siendo estos concebidos entre el 2015 y 2017.

        \begin{table}[htb]
            \centering
            \resizebox{\textwidth}{!}{
                \begin{tabular}{|p{0.3\textwidth}|p{0.3\textwidth}|p{0.5\textwidth}|p{0.5\textwidth}|}
                    \hline
                    \textbf{Nombre} & \textbf{Plataforma} & \textbf{Propósito} & \textbf{Características} \\
                    \hline
                        @HotelReviewsBot\cite{racketracer} & Python, NLTK & Consultar cualidades de un Hotel registrado en http://www.hostelworld.com/ & Capturaba tweets que contenían enlaces a un hotel en el referido portal y extraía comentarios relevantes del hospedaje, ya sea sobre el wi-fi, desayuno, baño, ducha o ruido del lugar. \\
                        \hline
                        A Tool of Conversation: Chatbot\cite{a_tool_of_conversation} & Java, Java Applets, Base de Datos & Demostrar la implementación de un flujo conversacional a partir de un esquema de patrones & Sugiere el uso de patrones para encontrar preguntas ya registradas en la base de datos y de hallarla, dar una respuesta al usuario. \\
                        \hline
                        Chatbots Enlatados\cite{Techlabs_2018} & Computación en la Nube & Proveer a empresas soluciones listas para su uso & Tienden a estar asociadas a ciertas industrias de manera predefinidas, tales como restaurantes y tiendas en línea. \\
                        \hline
                        Practical Sec2Sec\cite{Ramamoorthy2} & Python, NLTK, Tensorflow & Chatbot capaz de dar respuestas coloquiales, a partir de lo aprendido de corpus como Twitter y Cornell Movies & Implementa una red neuronal utilizando Tensorflow la cual es entrenada a partir de un corpus de conversaciones. \\
                    \hline
                \end{tabular}
            }
            \caption{Estado del arte para implementaciones de Chatbots.}
        \end{table}

    \section{Corpus y Preprocesamiento de la Data}
        El corpus utilizado para entrenar esta red corresponde a un conjunto de más de 700 mil mensajes procedentes de la red social Twitter. Este corpus es provisto por el repositorio chat\_corpus\cite{Ma_2020} donde detalla que para las líneas impares corresponden a \emph{tweets} o mensajes emitidos y las líneas pares corresponden a respuestas al  \emph{tweet} predecesor.
        
        Para modelar este vasto conjunto de datos fue preciso realizar una serie de pasos con la intención de reducir la complejidad de aprendizaje para la red a entrenar, pues el modelo Seq2Seq trae consigo varios retos y uno de ellos es la variabilidad de la longitud de la secuencias, por el hecho de que la longitud de las secuencias de entrada a procesar es indiscutiblemente variable. Otro problema destacable es la dimención del vocabulario a utilizar ya que sin duda alguna es ineficiente aplicar una función softmax para cada palabra del vocabulario con el cual trabajaremos\cite{Ramamoorthy}.
        
        \subsection{Conversión a minúscula}
        La primera tarea de preprocesado del corpus, todas las palabras en el mismo son llevadas a minúsculas con el objetivo de que palabras lexicográficamente equivalentes sean en un posterior paso consideradas al momento de evaluar su distribución.
        
        \subsection{Filtrado de caracteres}
        Cada carácter dentro para cada palabra es filtrado mediante una expresión regular de forma $[\text{a-z0-9}\backslash{s}]$, permitiéndonos tan solo procesar texto alfanumérico, evitando que la red considere escenarios complejos donde haya presencia de símbolos.
        
        \subsection{Ternas (Bucketing)}
        Es preciso para la entrenar una red Seq2Seq el que las secuencias de entrada sean de una misma longitud, siendo esto un inconveniente pues las secuencias en el corpus son variables en cuanto a longitud se trata. Para solventar esta situación existe lo conocido como ternas o “bucketing”, donde se elige un subconjunto tanto de preguntas como de respuestas las cuales entren dentro del rango establecido.
        
        Para el problema a tratar se tuvo el siguiente criterio, una terna o bucket $((0,20),(3,20))$ siendo el primer binomio el rango para las preguntas y el segundo el de respuestas, leído de forma: Una pregunta o secuencia de entrada debe de corresponder a una longitud no mayor a 20 palabras, mientras que una respuesta debe de constar entre tres y veinte palabras.
        
        \subsection{Indexado de Palabras}
        Este paso es uno de los más importantes y críticos de la implementación, pues se trata de crear una tabla de búsqueda en donde se tenga una distribución de frecuencia para cada palabra obtenida del paso anterior, siendo la primera en la lista la  palabra más utilizada y lo opuesto para la última palabra, dígase la menos frecuente. Asimismo, en la implementación estudiada la tabla obtenida es delimitada a tan solo las primeras ochol mil palabras, por lo que se restringe el dominio del vocabulario a tal cantidad.
        
        La implementación de ésta técnica es lograda apoyándose en la ya expuesta librería NLTK y su función \emph{FreqDist}, y se ha de destacar que una vez obtenida la distribución de frecuencia para cada palabra en el corpus y restringido el dominio del vocabulario, se añaden en la cabecera de la lista los caracteres ‘\_’ que representan rellenado o “padding” y el caracter “unk” o desconocido para palabras fueras del vocabulario obtenido; estos caracteres sirven de apoyo a la red pues como se explicará en el apartado siguiente, es preciso tener tales caracteres para situaciones excepcionales a la cual nos enfrentaremos.
        
        \subsection{Rellenado de Oraciones (Padding)}
        Como bien fue mencionado en el apartado “ternas”, es necesario al momento de entrenar que las secuencias utilizadas en la red  ocupen la misma dimensión, considerando el hecho de que existen secuencias de longitudes inferiores a veinte, siendo veinte el límite superior tanto para pregunta como respuesta. Es por esto que cada vector de secuencia el cual no alcance la dimensión mencionada para cada conjunto es acotado a tal dimensión, dígase veinte unidades, donde para las secuencias con menos de veinte unidades son rellenadas hasta alcanzar la dimensión en cuestión.


    \section{Arquitectura de la Red Seq2Seq}
        Para la creación de una red Seq2Seq como ya hemos mencionado, es utilizada la librería TensorLayer, la cual nos brinda una interfaz con el nombre propio de la red y el uso de la misma se muestra a continuación.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{img/code/seq2seq_model.png}
            \caption{Creación de modelo Seq2Seq mediante TensorLayer.}
        \end{figure}
        
        Una vez creado el modelo, a lo interno del mismo se tiene la siguiente configuración seguido del detalle de cada pieza.
    
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{img/code/seq2seq_arq.png}
            \caption{Arquitectura del modelo Seq2Seq empleado.}
        \end{figure}
        
        \begin{itemize}
            \item Embedding Layer(vocabulary\_size=8004, embedding\_size=1024).
            La capa de contextualización de palabras o \emph{Word Embedding}, se utiliza previo a pasar el vector de ID de palabras de la secuencia de entrada al codificador. También se usa previo a pasar el token especial “start\_id" al decodificador.
            \item Codificador:
                \begin{itemize}
                    \item Esta conformado por tres capas GRU (GRUCell), de 256 unidades cada una.
                    \item Convierte las palabras de la secuencia de entrada en vectores de estados ocultos. Cada vector de estado oculto representa una palabra y el contexto de esta.
                \end{itemize}
            \item Decodificador:
                \begin{itemize}
                    \item Tambien esta conformado por tres capas GRU (GRUCell), de 256 unidades cada una.
                \item La capa inicial recibe de entrada un vector con el token especial “start\_id" y se le pasan los estados ocultos del codificador como los estados iniciales.
                \item El codificador predice palabra por palabra la secuencia de salida, recibiendo en cada paso en el tiempo o instante T:
                \begin{enumerate}
                    \item Un contexto de todas las palabras hasta ese instante.
                    \item Los estados ocultos producidos hasta ese instante.
                \end{enumerate}
                \item Para maximizar la probabilidad de la palabra producida se utiliza el algoritmo de búsqueda Beam Search con número de 3 alternativas definido por defecto, también conocido como \emph{“top n"} o \emph{“beam width"}.
                \item La computación termina cuando se produce el número de palabras definido como threshold. Para este caso fue una secuencia de máximo 20 palabras.
                \end{itemize}
            \item Reshape Layer (shape(-1,256)), transforma la salida del decodificador en un vector para poder pasar la salida a la capa densa.
            \item Dense Layer de 8004 unidades, extrae las características de alto nivel aprendidas por el decodificador.
            \item Reshape Layer (shape(-1, 1, 8004)), transforma la salida de la capa densa en una distrubución de probabilidad de las palabras del vocabulario (8004 palabras).
        \end{itemize}
        
        
        La salida del codificador es un vector de 20 palabras a partir del cual se produce la salida final, concatenando palabra por palabra hasta encontrar el token especial \emph{“end\_id"}.

    \section{Entrenamiento del modelo}
        Para llevar a cabo el entrenamiento de la red neuronal ya descrita, es utilizado el corpus preprocesado resultante de apartados anteriores, donde tal corpus es segmentado en las siguientes proporciones: un 70\% del conjunto de datos es utilizado para entrenamiento, un 15\% para prueba y el 15\% restante para validación\cite{Britz_2015}.

        Asimismo, el entrenamiento de la red es realizado durante una serie de 50 epochs y en cada epoch le es suministrado como ejemplo un subconjunto o batch de 32 instancias. 
        
        Como ya se mencionó en apartados anteriores, se utilizan dos tokens especiales como prefijo y sufijo de cada secuencia, “start\_id” y “end\_id" respectivamente; donde a la hora de realizar sampling, “start\_id" es el token que representa el inicio de la secuencia, y “end\_id", representa el final, lo cual permite saber cuando terminar la operación de sampling.
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{img/tensorboard.png}
            \caption{Histórico del entrenamiento de la red.}
        \end{figure}

        Posteriormente se evalúa las predicciones del modelo a partir de la salida del mismo y se calcula la pérdida mediante el cálculo de entropía cruzada utilizando la función cross\_entropy\_seq\_with\_mask provista por tensorflow, el valor computado como pérdida es utilizado para calcular el ajuste del gradiente para los pesos de la red y posteriormente es aplicado a tales pesos, esto ocurre durante cada iteración o epoch.

    \section{Diálogo con el Modelo (Inference)}
        Tras el oportuno entrenamiento del modelo este es evaluado basicamente proporcionando un texto o secuencia de entrada, en relacion a la cual el modelo genera una secuencia como respuesta. Se ha de destacar que todo esto es apoyado en la tabla de indexado que se construye en la fase de modelado del corpus, donde la entrada digitada es llevada a su representación numérica, y la salida de la red es llevada a texto por el proceso inverso, buscando la equivalencia de cada índice con una palabra, y la unión de este vector es la secuencia producto legible por el usuario.
        
        A continuación se muestra una tabla donde se ilustran las preguntas y respuestas obtenidas por quienes elaboraron este proyecto. Es importante resaltar que el dominio de las preguntas y respuestas se circunscribe al contenido observado en el corpus.

    \begin{table}[htb]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|p{0.6\textwidth}|p{0.6\textwidth}|}
            \hline
            \textbf{Pregunta} & \textbf{Respuesta} \\
            \hline
            donald trump won last nights presidential debate according to snap online polls & thought he was a joke \\
            \hline
            trump campaign please never delete this & he was a joke \\
            \hline
            s new scoop is unk the castro connection how trumps company violated the us embargo against cuba & trump is a disgrace to the debate and the media is not a liar \\
            \hline
            who won the first presidential debate & trump will be a better time for a man \\
            \hline
            just wanna live in unk everything is 10x better there & i was just on the same side and i was like it was a good time \\
            \hline
        \end{tabular}
    }
    \caption{Ejemplo de preguntas y Respuestas para el Chatbot.}
    \end{table}
    
    \section{Conclusiones}
    [PENDIENTE]
    \pagebreak
    % \addcontentsline{toc}{section}{Bibliografía}
    \printbibliography[title={Bibliografía}]
\end{document}